# Autonomous Agents Framework (iasa_agente)

## Phases 3-13: From Reactive Behaviors to Reinforcement Learning

A comprehensive Python implementation of progressively sophisticated autonomous agent architectures, demonstrating the evolution from simple reactive behaviors to advanced learning systems.

**Part of**: [Artificial Intelligence for Autonomous Systems (IASA47094)](../README_IASA_Main.md)

---

## ğŸ“‹ Overview

This project implements **11 development phases** (3-13) covering the complete spectrum of agent design paradigms:

| Phase | Name | Focus | Algorithms |
|---|---|---|---|
| **3-5** | Reactive Behaviors | Agent responds to stimuli | Hierarchy, Suppression, Memory |
| **6-8** | State Space Search | Problem-solving with search | BFS, DFS, A*, Greedy |
| **9-10** | Deliberative Planning | Multi-step planning | PlanPEE, PlanPDM |
| **11-12** | MDPs | Decision under uncertainty | Utility iteration |
| **13** | Learning | Adaptive behavior | Q-Learning |

---

## ğŸ—‚ï¸ Directory Structure

```
iasa_agente/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ controlo_react/              # Phases 3-5: Reactive Control
â”‚   â”‚   â”œâ”€â”€ controloreact.py         # Main reactive controller
â”‚   â”‚   â””â”€â”€ reacoes/                 # Behavior implementations
â”‚   â”‚       â”œâ”€â”€ aproximar/           # Approach behavior module
â”‚   â”‚       â”œâ”€â”€ evitar/              # Avoidance behavior module
â”‚   â”‚       â”œâ”€â”€ explorar.py          # Exploration behavior
â”‚   â”‚       â”œâ”€â”€ resposta/            # Response behavior
â”‚   â”‚       â””â”€â”€ recolher.py          # Collection behavior
â”‚   â”‚
â”‚   â”œâ”€â”€ controlo_delib/              # Phases 9-10: Deliberative Control
â”‚   â”‚   â”œâ”€â”€ controlodelib.py         # Deliberative controller
â”‚   â”‚   â”œâ”€â”€ modelomundo.py           # World model representation
â”‚   â”‚   â”œâ”€â”€ operadormover.py         # Movement operators
â”‚   â”‚   â””â”€â”€ planeador.py             # Planning interface
â”‚   â”‚
â”‚   â”œâ”€â”€ controlo_aprend/             # Phase 13: Learning Control
â”‚   â”‚   â”œâ”€â”€ controloaprendref.py     # Learning control integration
â”‚   â”‚   â””â”€â”€ mecaprend.py             # Learning mechanism
â”‚   â”‚
â”‚   â”œâ”€â”€ lib/                         # Core AI Libraries
â”‚   â”‚   â”œâ”€â”€ pee/                     # Phases 6-8: State Space Search
â”‚   â”‚   â”‚   â”œâ”€â”€ estado.py            # State representation
â”‚   â”‚   â”‚   â”œâ”€â”€ operador.py          # Operator (state transition)
â”‚   â”‚   â”‚   â”œâ”€â”€ problema.py          # Problem definition
â”‚   â”‚   â”‚   â”œâ”€â”€ fronteira.py         # Search frontier (FIFO/LIFO/Priority)
â”‚   â”‚   â”‚   â”œâ”€â”€ mecanismoprocura.py  # Search mechanism base
â”‚   â”‚   â”‚   â”œâ”€â”€ busca_profundidade.py # DFS implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ busca_largura.py     # BFS implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ busca_custo_uniforme.py # Uniform cost
â”‚   â”‚   â”‚   â”œâ”€â”€ busca_sfrega.py      # Greedy search
â”‚   â”‚   â”‚   â”œâ”€â”€ busca_astar.py       # A* search
â”‚   â”‚   â”‚   â””â”€â”€ procura_grafo.py     # Graph search with cycle detection
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ pdm/                     # Phases 11-12: Markov Decision Process
â”‚   â”‚   â”‚   â”œâ”€â”€ mdp.py               # MDP model
â”‚   â”‚   â”‚   â”œâ”€â”€ solucionadorpdm.py   # MDP solver
â”‚   â”‚   â”‚   â”œâ”€â”€ utilidade.py         # Utility calculation
â”‚   â”‚   â”‚   â””â”€â”€ politica.py          # Policy representation
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ aprend_ref/              # Phase 13: Reinforcement Learning
â”‚   â”‚   â”‚   â”œâ”€â”€ aprendref.py         # Reinforcement learning base
â”‚   â”‚   â”‚   â”œâ”€â”€ aprendq.py           # Q-Learning implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ memoriaaprend.py     # Learning memory interface
â”‚   â”‚   â”‚   â”œâ”€â”€ memoriaesparsa.py    # Sparse Q-value storage
â”‚   â”‚   â”‚   â”œâ”€â”€ selaccao.py          # Action selection interface
â”‚   â”‚   â”‚   â”œâ”€â”€ selaccaoegreedy.py   # Îµ-Greedy strategy
â”‚   â”‚   â”‚   â””â”€â”€ selaccaomaxima.py    # Maximal action selection
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ plan/                    # Planning algorithms
â”‚   â”‚   â”‚   â”œâ”€â”€ planpee.py           # State-space planner
â”‚   â”‚   â”‚   â”œâ”€â”€ planpdm.py           # MDP-based planner
â”‚   â”‚   â”‚   â””â”€â”€ planejador.py        # Planner interface
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ ecr/                     # Behavior Control Libraries
â”‚   â”‚   â”‚   â”œâ”€â”€ comportamento.py     # Behavior base class
â”‚   â”‚   â”‚   â”œâ”€â”€ comportamento_ativo.py # Active behavior
â”‚   â”‚   â”‚   â””â”€â”€ compilador_comportamentos.py # Behavior composition
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ mod/                     # Model Components
â”‚   â”‚   â”‚   â”œâ”€â”€ componente.py        # Component base class
â”‚   â”‚   â”‚   â”œâ”€â”€ agente.py            # Agent interface
â”‚   â”‚   â”‚   â”œâ”€â”€ ambiente.py          # Environment interface
â”‚   â”‚   â”‚   â””â”€â”€ simulador.py         # Simulation framework
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ sae/                     # System for Execution Environments
â”‚   â”‚       â”œâ”€â”€ sae_doc.pdf          # SAE documentation
â”‚   â”‚       â”œâ”€â”€ ambiente_exec.py     # Execution environment
â”‚   â”‚       â””â”€â”€ simulador_sae.py     # SAE simulator integration
â”‚   â”‚
â”‚   â””â”€â”€ teste/                       # Test files and examples
â”‚       â”œâ”€â”€ teste_busca.py           # Search algorithm tests
â”‚       â”œâ”€â”€ teste_pdm.py             # MDP solver tests
â”‚       â”œâ”€â”€ teste_aprendq.py         # Q-Learning tests
â”‚       â””â”€â”€ teste_comportamentos.py  # Behavior tests
â”‚
â”œâ”€â”€ .env                             # Environment configuration
â”œâ”€â”€ .idea/                           # IDE settings (PyCharm)
â””â”€â”€ requirements.txt                 # Python dependencies
```

---

## ğŸ¯ Project Phases Detail

### **Phase 3-5: Reactive Behavior Control**

**Objective**: Implement hierarchical reactive behaviors with memory integration.

#### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Reactive Controller       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Behavior Hierarchy:         â”‚
â”‚ â”œâ”€ EvitarObstaculo (HIGH)  â”‚
â”‚ â”œâ”€ AproximarAlvo (MEDIUM)  â”‚
â”‚ â””â”€ Explorar (LOW)           â”‚
â”‚                             â”‚
â”‚ Memory: Visited locations   â”‚
â”‚ Goal: Collect targets      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Key Components

**1. ControloReact** (`controloreact.py`)
- Manages behavior hierarchy
- Selects active behavior based on priority
- Integrates memory system
- Executes action determined by active behavior

**2. Behavior Modules** (`reacoes/`)
- **Explorar**: Random exploration with memory
- **AproximarAlvo**: Approach nearest target with collision awareness
- **EvitarObstaculo**: Avoid obstacles with priority suppression
- **Recolher**: Collect/interact with targets

**3. Memory System**
- Tracks visited locations
- Prevents infinite loops
- Maintains state across cycles

#### Key Concepts

**Behavior Hierarchy**:
```
If obstacle detected â†’ Execute EvitarObstaculo (suppresses lower behaviors)
Else if target visible â†’ Execute AproximarAlvo
Else â†’ Execute Explorar (default)
```

**Suppression Mechanism**:
- Higher priority behaviors can inhibit lower ones
- Prevents conflicting actions
- Enables coordinated composite behaviors

**Memory Integration**:
- Agents remember visited locations
- Prevents re-exploration of known areas
- Improves exploration efficiency

---

### **Phase 6-8: State Space Search**

**Objective**: Implement search algorithms for problem-solving in deterministic environments.

#### Problem Model

**State Representation** (`estado.py`)
```python
state = (x, y, facing_direction)  # Complete system configuration
```

**Operators** (`operador.py`)
```python
operator = (action, preconditions, cost, effects)
# Example: Move north costs 1, changes (x,y) coordinates
```

**Problem Definition** (`problema.py`)
```python
problem = (initial_state, operators, goal_state)
```

#### Search Algorithms Implemented

**1. Uninformed Search** (No domain knowledge)

| Algorithm | Optimal | Complete | Space | Time | Notes |
|---|---|---|---|---|---|
| **BFS** | âœ“ | âœ“ | O(b^d) | O(b^d) | Explores level-by-level |
| **DFS** | âœ— | Limited | O(d) | O(b^d) | Memory efficient |
| **Depth-Limited** | âœ— | âœ— | O(d) | O(b^d) | Prevents infinite loops |
| **Iterative Deepening** | âœ“ | âœ“ | O(d) | O(b^d) | Combines BFS+DFS |

**2. Informed Search** (Uses heuristics)

| Algorithm | Optimal | Complete | Strategy |
|---|---|---|---|
| **Uniform Cost** | âœ“ | âœ“ | Minimize path cost |
| **Greedy** | âœ— | âœ— | Minimize estimated remaining cost |
| **A\*** | âœ“ | âœ“ | g(n) + h(n) = cost + heuristic |

#### Key Files

- `busca_profundidade.py` - Depth-First Search
- `busca_largura.py` - Breadth-First Search
- `busca_custo_uniforme.py` - Uniform cost search
- `busca_astar.py` - A* with heuristics
- `procura_grafo.py` - Graph search with cycle detection

#### Frontier Management

```
FIFO (Queue) â†’ Breadth-First
LIFO (Stack) â†’ Depth-First
Priority Queue â†’ Uniform Cost / A*
```

---

### **Phase 9-10: Deliberative Planning**

**Objective**: Build agents that plan multi-step solutions before execution.

#### Agent Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Perception                     â”‚
â”‚   (Read current world state)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   World Model Update             â”‚
â”‚   (Maintain belief state)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Planning (Deliberation)        â”‚
â”‚   - PlanPEE: A* search          â”‚
â”‚   - PlanPDM: MDP solver         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Plan Execution                 â”‚
â”‚   (Follow computed plan)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Replanning Check               â”‚
â”‚   (Replan if world changed)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Planner Types

**1. PlanPEE** (`planpee.py`) - State-Space Planner
- Uses A* search to find action sequence
- Creates explicit plan before execution
- Replans when environment deviates from model

**2. PlanPDM** (`planpdm.py`) - MDP-Based Planner
- Uses probabilistic model of environment
- Computes expected utilities per action
- Handles stochastic transitions

#### World Model

**ModeloMundo** (`modelomundo.py`)
- Represents agent's beliefs about environment
- Updated with perceptions
- Used for lookahead during planning
- Independent of actual environment

#### Planning Process

```
1. Current State (perception)
2. Goal Definition (what to achieve)
3. Lookahead Simulation (what-if analysis)
4. Operator Application (possible actions)
5. Plan Generation (sequence of actions)
6. Plan Execution (follow plan)
7. Monitoring (check if still valid)
8. Replanning (if necessary)
```

---

### **Phase 11-12: Markov Decision Processes**

**Objective**: Model decision-making under uncertainty and probabilistic environments.

#### MDP Formalism

**Components** (`mdp.py`)
```python
MDP = {
    states: S,                  # Set of all possible states
    actions: A(s),              # Available actions per state
    transitions: P(s'|s,a),    # Probability of outcome
    rewards: R(s,a),           # Immediate payoff
    gamma: Î³                    # Discount factor (0 < Î³ â‰¤ 1)
}
```

#### Utility Calculation (`utilidade.py`)

**Bellman Equation**:
```
U(s) = max_a [ Î£ P(s'|s,a) Ã— (R(s,a) + Î³ Ã— U(s')) ]
```

**Iterative Algorithm**:
1. Initialize utilities (U = 0)
2. Repeat until convergence:
   - For each state: Update U(s) using Bellman equation
   - Check if max difference < Îµ (convergence threshold)
3. Return final utility values

#### Policy Extraction (`politica.py`)

Once utilities converge:
```python
policy(s) = argmax_a [ Î£ P(s'|s,a) Ã— (R(s,a) + Î³ Ã— U(s')) ]
```

#### Parameter Impact

| Parameter | Low Value | High Value | Effect |
|---|---|---|---|
| **Î³ (discount)** | 0 | 1 | Myopic â† â†’ Far-sighted |
| **Î± (learning rate)** | 0 | 1 | Slow â† â†’ Fast updates |
| **Îµ (exploration)** | 0 | 1 | Greedy â† â†’ Random |

---

### **Phase 13: Reinforcement Learning**

**Objective**: Learn optimal behavior through trial-and-error without world model.

#### Q-Learning Algorithm (`aprendq.py`)

**Q-Value**: Estimated utility of action in state
```
Q(s,a) = expected future discounted reward for action a in state s
```

**Learning Rule** (Temporal Difference):
```
Q(s,a) â† Q(s,a) + Î± Ã— [r + Î³ Ã— max_a' Q(s',a') - Q(s,a)]
         â””â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         old value       prediction error (TD error)
```

#### Components

**1. AprendQ** (`aprendq.py`)
- Core Q-Learning algorithm
- Updates Q-values based on experience
- Maintains learning statistics

**2. Memory System** (`memoriaesparsa.py`)
- Stores Q-values for state-action pairs
- Only stores non-zero values (sparse)
- Efficient for large state spaces

**3. Action Selection** (`selaccaoegreedy.py`)
- Îµ-Greedy strategy
- Probability Îµ: random action (exploration)
- Probability 1-Îµ: best known action (exploitation)
- Epsilon decays over time for convergence

#### Learning Process

```
Initialize: Q(s,a) = 0 for all s,a

Repeat for each episode:
    s = initial_state
    Repeat for each step in episode:
        a = select_action(s) using Îµ-Greedy
        Execute a, observe reward r and next state s'
        Q(s,a) â† Q(s,a) + Î± Ã— [r + Î³ Ã— max_a' Q(s',a') - Q(s,a)]
        s â† s'
    Until s is terminal
Until convergence
```

#### Key Parameters

| Parameter | Typical Range | Effect |
|---|---|---|
| **Learning Rate (Î±)** | 0.01 - 0.1 | Fast vs. slow convergence |
| **Discount (Î³)** | 0.9 - 0.99 | Future reward importance |
| **Initial Epsilon (Îµ)** | 0.1 - 1.0 | Starting exploration level |
| **Epsilon Decay** | 0.995 - 0.9999 | How fast to reduce exploration |

---

## ğŸ”„ Environment Integration

### SAE Framework

**Sistema de Ambientes de ExecuÃ§Ã£o** (System for Execution Environments)

The SAE provides:
- **Environment Simulation**: Grid worlds, continuous spaces
- **Agent Execution**: Timestep-based or event-based
- **Visualization**: Real-time display of agent behavior
- **Data Logging**: Trajectory and reward tracking

**Usage**:
```python
from lib.sae import ambiente_exec
env = ambiente_exec.AmbienteSAE(grid_size=10, obstacles=True)
agent = MeuAgente(env)
agent.executar(timesteps=1000)
```

### Integration Patterns

**Phase 3-5**: Behaviors interact with SAE environment
**Phase 6-8**: Search uses problem model of environment
**Phase 9-10**: Planning with world model, execution in SAE
**Phase 11-12**: MDP solver for SAE environment
**Phase 13**: Q-Learning agents in SAE environment

---

## ğŸ’» Development Guide

### Running Search Algorithms (Phases 6-8)

```python
from src.lib.pee import problema, busca_astar

# Define problem
state_initial = (0, 0)
state_goal = (5, 5)
problem = problema.Problema(state_initial, state_goal)

# Solve with A*
search = busca_astar.BuscaAStar(heuristica=manhattan_distance)
solution = search.resolver(problem)

print(f"Path found: {solution.caminho}")
print(f"Cost: {solution.custo}")
print(f"Nodes explored: {solution.nos_explorados}")
```

### Running Planning (Phases 9-10)

```python
from src.controlo_delib import controlodelib
from src.lib.plan import planpee

# Create deliberative controller
planeador = planpee.PlanPEE()
controlo = controlodelib.ControloDelib(planeador)

# In agent loop
for timestep in range(1000):
    percepcao = ambiente.percepcao()
    controlo.atualizar_mundo(percepcao)
    accao = controlo.deliberar(objetivo)
    ambiente.executar(accao)
```

### Running Q-Learning (Phase 13)

```python
from src.controlo_aprend import controloaprendref
from src.lib.aprend_ref import aprendq, selaccaoegreedy

# Create Q-Learning agent
memoria = memoriaesparsa.MemoriaEsparsa()
aprendizado = aprendq.AprendQ(memoria, alpha=0.1, gamma=0.99)
seleccao = selaccaoegreedy.SelAccaoEGreedy(epsilon=0.1)
controlo = controloaprendref.ControloAprendRef(aprendizado, seleccao)

# Training loop
for episode in range(1000):
    state = ambiente.reset()
    for step in range(max_steps):
        action = controlo.seleccionar_accao(state)
        state, reward = ambiente.step(action)
        controlo.aprender(state, action, reward, state)

# Use learned policy
state = ambiente.reset()
while not done:
    action = controlo.melhor_accao(state)  # Exploit learned policy
    state, reward = ambiente.step(action)
```

---

## ğŸ§ª Testing & Validation

Test files in `src/teste/`:
- `teste_busca.py` - Validates search algorithms
- `teste_pdm.py` - Validates MDP solver
- `teste_aprendq.py` - Validates Q-Learning
- `teste_comportamentos.py` - Validates behaviors

Run tests:
```bash
python -m pytest src/teste/ -v
```

---

## ğŸ“Š Performance Characteristics

### Search Algorithms (Phases 6-8)

| Algorithm | Time | Space | Optimal | Use Case |
|---|---|---|---|---|
| BFS | O(b^d) | O(b^d) | âœ“ | Small graphs, shortest path |
| DFS | O(b^d) | O(d) | âœ— | Memory constrained |
| A* | O(b^d) | O(b^d) | âœ“ | Heuristic available |
| Greedy | O(b log b) | O(b) | âœ— | Speed critical |

### Planning (Phases 9-10)

| Planner | Complexity | Stochastic | Use Case |
|---|---|---|---|
| PlanPEE | O(search complexity) | No | Deterministic environments |
| PlanPDM | O(statesÂ² Ã— iterations) | Yes | Uncertain environments |

### Learning (Phase 13)

| Aspect | Value | Notes |
|---|---|---|
| Convergence | O(1/iterations) | Slower than planned agents |
| Memory | O(states Ã— actions) | Sparse representation efficient |
| Time/Step | O(1) | Constant action selection |

---

## ğŸ“ Learning Outcomes

After completing this project, you understand:

1. âœ“ Reactive behavior hierarchies and suppression
2. âœ“ State space search with various strategies
3. âœ“ Graph search with cycle detection
4. âœ“ Heuristic design (admissibility, consistency)
5. âœ“ Planning with world models and lookahead
6. âœ“ Markov Decision Processes and utility
7. âœ“ Q-Learning and temporal difference learning
8. âœ“ Exploration-exploitation trade-off
9. âœ“ Architecture progression from reactive to learning
10. âœ“ Software engineering for AI systems

---

## ğŸ”— See Also

- [Main IASA Repository README](../README_IASA_Main.md)
- [Java Game Agent README](../iasa_jogo/README.md)
- [SAE Framework Documentation](./src/lib/sae/sae-doc.pdf)
- [Final Academic Report](../iasa47094relatorio.pdf)

---

## ğŸ“ Citation

If using this project for academic work, cite as:

```bibtex
@thesis{azevedo2022iasa,
  author = {Azevedo, Pedro},
  title = {Artificial Intelligence for Autonomous Systems},
  school = {Instituto Superior de Engenharia de Lisboa},
  year = {2022},
  course = {IASA47094}
}
```

---

**Author**: Pedro Azevedo (A47094)  
**Institution**: ISEC - Instituto Superior de Engenharia de Lisboa  
**Course**: Artificial Intelligence for Autonomous Systems (IASA47094)  
**Status**: Complete - All 13 phases implemented  
**Last Updated**: January 29, 2026
