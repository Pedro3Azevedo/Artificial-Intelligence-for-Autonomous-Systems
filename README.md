# Artificial Intelligence for Autonomous Systems (IASA)

A comprehensive academic project demonstrating the progression from reactive agents to sophisticated autonomous systems with planning and reinforcement learning. Built for the "Artificial Intelligence for Autonomous Systems" course (IASA47094) at ISEC - Instituto Superior de Engenharia de Lisboa.

**Repository**: [Pedro3Azevedo/Artificial-Intelligence-for-Autonomous-Systems](https://github.com/Pedro3Azevedo/Artificial-Intelligence-for-Autonomous-Systems)

---

## ğŸ“‹ Project Overview

This repository contains **two integrated AI projects** totaling **13 development phases**:

### **Part 1: Animal Photography Game (Java) - Phases 1-2**
A reactive agent simulation where a virtual character autonomously photographs animals using state machine-based decision-making.

### **Part 2: Autonomous Agents Framework (Python) - Phases 3-13**
Progressive implementation of increasingly sophisticated agent architectures:
- **Reactive Behaviors** (Phases 3-5)
- **State Space Search** (Phases 6-8)
- **Deliberative Planning** (Phases 9-10)
- **Markov Decision Processes** (Phases 11-12)
- **Reinforcement Learning** (Phase 13)

---

## ğŸ—‚ï¸ Repository Structure

```
Artificial-Intelligence-for-Autonomous-Systems/
â”œâ”€â”€ iasa_jogo/                           # Phase 1-2: Java Game Agent
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ Jogo.java                    # Main game controller
â”‚       â”œâ”€â”€ Personagem.java              # Intelligent agent character
â”‚       â”œâ”€â”€ Ambiente.java                # Environment simulator
â”‚       â”œâ”€â”€ Controlo.java                # State machine control
â”‚       â”œâ”€â”€ Estado.java                  # State definition
â”‚       â”œâ”€â”€ MaquinaEstados.java          # Generic state machine
â”‚       â”œâ”€â”€ Transicao.java               # State transitions
â”‚       â”œâ”€â”€ Percecao.java                # Perception wrapper
â”‚       â”œâ”€â”€ Accao.java                   # Action enumeration
â”‚       â””â”€â”€ Evento.java                  # Event enumeration
â”‚
â”œâ”€â”€ iasa_agente/                         # Phase 3-13: Python Agents
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ controlo_react/              # Phase 3-5: Reactive Control
â”‚       â”‚   â”œâ”€â”€ controloreact.py
â”‚       â”‚   â””â”€â”€ reacoes/                 # Behavior implementations
â”‚       â”‚       â”œâ”€â”€ explorar.py          # Exploration
â”‚       â”‚       â”œâ”€â”€ aproximar/           # Approach behavior
â”‚       â”‚       â”œâ”€â”€ evitar/              # Avoidance behavior
â”‚       â”‚       â””â”€â”€ recolher.py          # Collection behavior
â”‚       â”‚
â”‚       â”œâ”€â”€ controlo_delib/              # Phase 9-10: Deliberative Control
â”‚       â”‚   â”œâ”€â”€ controlodelib.py
â”‚       â”‚   â”œâ”€â”€ modelomundo.py           # World model
â”‚       â”‚   â””â”€â”€ operadormover.py
â”‚       â”‚
â”‚       â”œâ”€â”€ controlo_aprend/             # Phase 13: Learning Control
â”‚       â”‚   â”œâ”€â”€ controloaprendref.py
â”‚       â”‚   â””â”€â”€ mecaprend.py
â”‚       â”‚
â”‚       â”œâ”€â”€ lib/                         # Core AI Libraries
â”‚       â”‚   â”œâ”€â”€ pee/                     # Phase 6-8: State Space Search
â”‚       â”‚   â”‚   â””â”€â”€ Search algorithms: BFS, DFS, A*, Greedy
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ pdm/                     # Phase 11-12: Markov Decision Processes
â”‚       â”‚   â”‚   â””â”€â”€ MDP solver
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ aprend_ref/              # Phase 13: Reinforcement Learning
â”‚       â”‚   â”‚   â”œâ”€â”€ aprendq.py           # Q-Learning
â”‚       â”‚   â”‚   â”œâ”€â”€ selaccaoegreedy.py   # Îµ-Greedy action selection
â”‚       â”‚   â”‚   â””â”€â”€ memoriaesparsa.py    # Sparse memory
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ plan/                    # Planning algorithms
â”‚       â”‚   â”œâ”€â”€ ecr/                     # Reactive behavior control
â”‚       â”‚   â”œâ”€â”€ mod/                     # Models library
â”‚       â”‚   â””â”€â”€ sae/                     # Execution environment
â”‚       â”‚
â”‚       â””â”€â”€ teste/                       # Test files
â”‚
â”œâ”€â”€ README.md                            # Main project README
â”œâ”€â”€ iasa47094relatorio.pdf               # Academic final report
â””â”€â”€ .gitignore
```

---

## ğŸ¯ Project Phases

### **Phase 1-2: Reactive Game Agent (Java)**
**Location**: `iasa_jogo/src/`

A reactive intelligent agent that autonomously photographs animals using state machines.

- **Architecture**: Perception â†’ State Machine â†’ Action
- **Components**: 10 Java classes implementing agent cycle
- **Key Concept**: Reactive agent with stateful behavior
- **Patterns**: State Machine design pattern

[ğŸ‘‰ **Detailed README**](./iasa_jogo/README.md)

---

### **Phase 3-5: Reactive Behavior Architecture (Python)**
**Location**: `iasa_agente/src/controlo_react/`

Hierarchical reactive behaviors with agent memory and obstacle avoidance.

- **Behaviors**: 
  - `Explorar` - Random exploration
  - `AproximarAlvo` - Approach targets with priority
  - `EvitarObst` - Obstacle avoidance
  - `Recolher` - Collection behavior
- **Key Concept**: Behavior hierarchy with suppression
- **Implementation**: Behavior modules composed into reactive controller
- **Memory Integration**: Prevents infinite loops through visited location tracking

---

### **Phase 6-8: State Space Search (Python)**
**Location**: `iasa_agente/src/lib/pee/`

Implementation of multiple search algorithms for problem-solving.

**Search Algorithms**:
| Algorithm | Optimal | Complete | Notes |
|---|---|---|---|
| Breadth-First (BFS) | âœ“ | âœ“ | Guarantees shortest path |
| Depth-First (DFS) | âœ— | Limited | Memory efficient |
| Iterative Deepening | âœ“ | âœ“ | Combines BFS+DFS benefits |
| Uniform Cost | âœ“ | âœ“ | Finds cheapest path |
| Greedy (Best-First) | âœ— | âœ— | Fast but suboptimal |
| A* | âœ“ | âœ“ | Optimal with admissible heuristic |

**Problem Model**:
- States: Unique configurations
- Operators: State transformations with costs
- Goal: Target state definition
- Search Space: Graph of reachable states

---

### **Phase 9-10: Deliberative Agent with Planning (Python)**
**Location**: `iasa_agente/src/controlo_delib/`, `iasa_agente/src/lib/plan/`

Agents that plan multi-step solutions using world models.

**Planner Types**:

1. **PlanPEE** (State-Space Planner)
   - Uses search algorithms (A*, Greedy, Uniform Cost)
   - Builds explicit plan before execution
   - Reconsiders when environment changes

2. **PlanPDM** (MDP-Based Planner)
   - Probabilistic decision-making
   - Balances exploration vs. exploitation
   - Handles stochastic environments

**Decision Cycle**:
```
Perceive â†’ Update World Model â†’ Deliberate (Plan) â†’ Execute â†’ Repeat
```

---

### **Phase 11-12: Markov Decision Processes (Python)**
**Location**: `iasa_agente/src/lib/pdm/`

Probabilistic framework for decision-making under uncertainty.

**Key Concepts**:
- **States**: Complete system configurations
- **Actions**: Available choices per state
- **Transition Probabilities**: Likelihood of outcomes
- **Rewards**: Immediate payoff per action
- **Discount Factor (Î³)**: Weights future vs. immediate rewards

**Utility Calculation**:
```
U(s) = max_a [ Î£ P(s'|s,a) Ã— (R(a,s) + Î³ Ã— U(s')) ]
```

**Impact of Parameters**:
- Î³ â‰ˆ 0: Immediate rewards preferred (short-sighted)
- Î³ â‰ˆ 1: Long-term planning (far-sighted)
- Higher discount â†’ Better convergence but more computation

---

### **Phase 13: Reinforcement Learning (Python)**
**Location**: `iasa_agente/src/controlo_aprend/`, `iasa_agente/src/lib/aprend_ref/`

Learning from experience without a world model.

**Algorithm**: Q-Learning (Off-Policy Temporal Difference)

**Components**:
- **AprendQ**: Q-Learning algorithm
- **MemoriaEsparsa**: Sparse Q-value storage (state-action pairs)
- **SelAccaoEGreedy**: Îµ-Greedy exploration strategy
- **ControloAprendRef**: Integration with control system

**Key Parameters**:
- **Learning Rate (Î±)**: How fast to update Q-values
- **Discount Factor (Î³)**: Future reward weighting
- **Exploration Rate (Îµ)**: Balance exploration vs. exploitation

**Q-Value Update**:
```
Q(s,a) â† Q(s,a) + Î± Ã— [r + Î³ Ã— max_a' Q(s',a') - Q(s,a)]
```

---

## ğŸ› ï¸ Technologies & Tools

### Languages
- **Java** (12.2% of codebase)
  - OOP, generics, design patterns
  
- **Python** (87.8% of codebase)
  - NumPy, SciPy for numerical computation
  - Object-oriented design

### Libraries & Frameworks
- **SAE (Sistema de Ambientes de ExecuÃ§Ã£o)**
  - Environment simulation and visualization
  - Agent execution framework
  - Integrated with all projects

### Development Tools
- **IDE**: PyCharm, VS Code, IntelliJ IDEA
- **Version Control**: Git/GitHub
- **Documentation**: PDF report (academic format)

---

## ğŸš€ Getting Started

### Prerequisites
- **Java** (for Phase 1-2): JDK 8+
- **Python** (for Phase 3-13): Python 3.7+
- **Git**: For cloning the repository

### Installation

```bash
# Clone repository
git clone https://github.com/Pedro3Azevedo/Artificial-Intelligence-for-Autonomous-Systems.git
cd Artificial-Intelligence-for-Autonomous-Systems

# Setup Python environment (for Phases 3-13)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt  # If available

# Setup Java (for Phases 1-2)
cd iasa_jogo
javac -d bin src/**/*.java
java -cp bin src.Jogo
```

---

## ğŸ“– Detailed Project Documentation

### Project 1: Java Game Agent
- **File**: [iasa_jogo/README.md](./iasa_jogo/README.md)
- **Content**: 
  - Reactive agent architecture
  - State machine implementation
  - Class structure and design patterns
  - Game flow and execution
  - How to compile and run

### Project 2: Python Autonomous Agents
- **File**: [iasa_agente/README.md](./iasa_agente/README.md) *(to be created)*
- **Content**:
  - Reactive behavior control
  - Search algorithms and problem-solving
  - Planning and world models
  - Markov Decision Processes
  - Reinforcement learning implementation
  - Integration guide

---

## ğŸ“š Learning Path

Follow this progression to understand the complete system:

1. **Start with Foundations** (Reactive Game)
   - Understand agent model: sensors â†’ brain â†’ actuators
   - Learn state machines for decision-making
   - See perception-action cycle in practice

2. **Move to Behaviors** (Reactive Architectures)
   - Composite behaviors and hierarchies
   - Suppression and priority mechanisms
   - Memory for stateful reactions

3. **Learn Problem-Solving** (State Space Search)
   - Graph traversal and tree search
   - Heuristics for guided search
   - Trade-offs: optimal vs. efficient

4. **Understand Planning** (Deliberative Agents)
   - World models and lookahead
   - Multi-step problem decomposition
   - Replanning under change

5. **Model Uncertainty** (Markov Decision Processes)
   - Probabilistic environments
   - Reward structure and utility
   - Iterative policy optimization

6. **Enable Learning** (Reinforcement Learning)
   - Trial-and-error adaptation
   - Q-value temporal difference
   - Exploration-exploitation balance

---

## ğŸ“Š Architecture Progression

```
Phase 1-2: Reactive Agent
â”œâ”€â”€ State Machine
â””â”€â”€ Deterministic State Transitions

      â†“

Phase 3-5: Reactive Behaviors
â”œâ”€â”€ Hierarchical Behaviors
â”œâ”€â”€ Memory Integration
â””â”€â”€ Priority-Based Composition

      â†“

Phase 6-8: Search & Problem-Solving
â”œâ”€â”€ Graph/Tree Search
â”œâ”€â”€ Heuristic-Guided Search
â””â”€â”€ Optimal Path Finding

      â†“

Phase 9-10: Planning
â”œâ”€â”€ World Model
â”œâ”€â”€ Lookahead Reasoning
â””â”€â”€ Reconsideration Mechanism

      â†“

Phase 11-12: Stochastic Decision-Making
â”œâ”€â”€ Markov Decision Process
â”œâ”€â”€ Probability Model
â””â”€â”€ Utility-Based Choice

      â†“

Phase 13: Learning
â”œâ”€â”€ Q-Learning Algorithm
â”œâ”€â”€ Exploration-Exploitation
â””â”€â”€ Adaptive Behavior
```

---

## ğŸ“ Course Context

**Course**: Artificial Intelligence for Autonomous Systems (IASA47094)  
**Instructor**: Engenheiro Lus Morgado  
**Institution**: ISEC - Instituto Superior de Engenharia de Lisboa  
**Duration**: One academic year with iterative phases  
**Assessment**: Final report + implementation + defense

---

## ğŸ“– Academic Report

A comprehensive final report document is included: **`iasa47094relatorio.pdf`**

**Report Contents**:
- Theoretical frameworks and AI concepts
- Implementation details for all phases
- Algorithm explanations and pseudocode
- Simulation results and visualizations
- Performance analysis and comparisons
- Limitations and future improvements
- Bibliography and webography

---

## âœ¨ Key Features

### Software Engineering
âœ“ Modular architecture with clear separation of concerns  
âœ“ Generic programming (Java generics, Python protocols)  
âœ“ Design patterns (State, Strategy, Composite)  
âœ“ Encapsulation and abstraction principles  
âœ“ Extensible framework for custom agents/behaviors  

### AI Concepts
âœ“ Reactive agent paradigm  
âœ“ State machine formalism  
âœ“ Search algorithms with complexity analysis  
âœ“ Planning under deterministic and stochastic conditions  
âœ“ Reinforcement learning with Q-Learning  

### Code Quality
âœ“ Well-documented with inline comments  
âœ“ Clear class hierarchies and interfaces  
âœ“ Separation between models, control, and execution  
âœ“ Reusable libraries for algorithm implementation  

---

## ğŸ”® Potential Extensions

### Immediate Next Steps
- [ ] Multi-agent coordination and communication
- [ ] Continuous state/action spaces
- [ ] Deep Q-Networks for complex problems
- [ ] Transfer learning between tasks

### Advanced Topics
- [ ] Policy gradient methods (Actor-Critic)
- [ ] Monte Carlo tree search (MCTS)
- [ ] Inverse reinforcement learning
- [ ] Hierarchical reinforcement learning

### Application Domains
- Autonomous robotics
- Game AI agents
- Resource management systems
- Navigation and pathfinding

---

## ğŸ“ Files & Artifacts

| File | Purpose |
|---|---|
| `iasa47094relatorio.pdf` | Complete academic final report |
| `README.md` | This file - repository overview |
| `iasa_jogo/README.md` | Java game agent documentation |
| `iasa_agente/README.md` | Python agents documentation (TBD) |
| `iasa_jogo/src/*.java` | 10 Java classes for game agent |
| `iasa_agente/src/**/*.py` | 50+ Python modules for agents |

---

## ğŸ’¡ Key Insights

1. **Reactive agents are fast but brittle** - No learning or planning; cannot handle novel situations
2. **Planning adds sophistication** - Lookahead enables multi-step problem-solving
3. **Stochastic models handle uncertainty** - MDPs provide principled decision-making under risk
4. **Learning improves over time** - Q-Learning adapts behavior without explicit programming
5. **Architecture determines capability** - Same environment, different control produces vastly different behaviors

---

## ğŸ¤ Contributing & Usage

This is an **academic project** for educational purposes. Feel free to:
- Study the code for learning
- Adapt for your own projects
- Reference in academic work
- Extend with new algorithms

---

## ğŸ“„ License

Educational use only. Project completed as coursework for IASA47094.

---

## ğŸ‘¤ Author

**Pedro Azevedo** (A47094)  
Master's Degree in Informatics Engineering and Multimedia  
ISEC - Instituto Superior de Engenharia de Lisboa

---

## ğŸ”— Related Resources

- **[IASA Course Materials](https://moodle.isec.pt)** - Course documentation and theory
- **[SAE Framework Documentation](./iasa_agente/src/lib/sae/sae-doc.pdf)** - Environment simulator docs
- **[Final Report](./iasa47094relatorio.pdf)** - Detailed academic analysis

---

**Last Updated**: January 29, 2026  
**Repository Created**: 2022  
**Status**: Complete (13 phases implemented)
